{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3eceb24b",
      "metadata": {
        "id": "3eceb24b"
      },
      "source": [
        "\n",
        "# Data-Centric ML — 500 Most Recent (Semantic Scholar, Scrape Only)\n",
        "\n",
        "This notebook **only** fetches the 500 most recent papers related to *data-centric machine learning* from the Semantic Scholar Graph API, using:\n",
        "- A **multi-query** strategy (since Boolean `OR` isn't supported like web search).\n",
        "- Strict **1 request/second** throttle with retry/backoff for 429/5xx.\n",
        "- Local **deduplication**, **date parsing**, sorting by recency, and trimming to **500** papers.\n",
        "- Output: `dcml_recent_500.csv` (title, abstract, publicationDate/year, venue, DOI, URL).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cad9fe5",
      "metadata": {
        "id": "9cad9fe5"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip -q install requests pandas tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d690e25b",
      "metadata": {
        "id": "d690e25b"
      },
      "source": [
        "\n",
        "## Set your Semantic Scholar API key (securely)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f32795e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f32795e7",
        "outputId": "076a421b-821a-432d-d63e-802392d43ce3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API key set in environment variable S2_API_KEY.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if \"S2_API_KEY\" not in os.environ or not os.environ[\"S2_API_KEY\"]:\n",
        "    os.environ[\"S2_API_KEY\"] = getpass(\"Enter your Semantic Scholar API key (input hidden): \")\n",
        "print(\"API key set in environment variable S2_API_KEY.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a6df6e7",
      "metadata": {
        "id": "1a6df6e7"
      },
      "source": [
        "\n",
        "## (Optional) Mount Google Drive\n",
        "Uncomment to save the CSV directly to Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7679a798",
      "metadata": {
        "id": "7679a798"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# OUTDIR = \"/content/drive/MyDrive/dcml_outputs\"\n",
        "# import os; os.makedirs(OUTDIR, exist_ok=True)\n",
        "# print(\"Using:\", OUTDIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f06cc16b",
      "metadata": {
        "id": "f06cc16b"
      },
      "source": [
        "\n",
        "## Fetch the most recent papers\n",
        "We issue several focused queries, paginate a few pages each, and merge locally.  \n",
        "Then we deduplicate (DOI + normalized title), parse dates, sort by recency, and keep the freshest 500 with nontrivial abstracts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67a84012",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67a84012",
        "outputId": "c10c455f-35cc-4a60-b4e4-5fd0e1e9b047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Fetching (multi-query, throttled) ...\n",
            "Query: data-centric machine learning\n",
            "  +100 (total=100)\n",
            "  +100 (total=200)\n",
            "  +100 (total=300)\n",
            "  +100 (total=400)\n",
            "  +100 (total=500)\n",
            "  +100 (total=600)\n",
            "  +100 (total=700)\n",
            "  +100 (total=800)\n",
            "  +100 (total=900)\n",
            "  +100 (total=1000)\n",
            "Raw items fetched: 1000\n",
            "After to_rows: 762\n",
            "Unique DOIs: 717\n",
            "Missing publicationDate: 28 | Missing year: 0\n",
            "Abstract length ≥ 80: 762\n",
            "Kept 38 most recent with nontrivial abstracts from CS venues.\n",
            "Saved: dcml_recent_1000.csv\n"
          ]
        }
      ],
      "source": [
        "import os, time, re, warnings\n",
        "from datetime import datetime\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "S2_API_KEY = os.getenv(\"S2_API_KEY\")\n",
        "assert S2_API_KEY, \"Please set S2_API_KEY in your environment.\"\n",
        "HEADERS = {\"x-api-key\": S2_API_KEY}\n",
        "SEARCH_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "\n",
        "FIELDS = \"title,abstract,year,venue,publicationDate,externalIds,url\"\n",
        "PAGE_LIMIT = 100\n",
        "TARGET = 1000\n",
        "\n",
        "# Focused queries covering the DCML landscape\n",
        "QUERIES = [\n",
        "    \"data-centric machine learning\"\n",
        "]\n",
        "\n",
        "def robust_get(url, headers, params, sleep_s=1.1, max_retries=6):\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        time.sleep(sleep_s)  # pre-request throttle for 1 req/sec\n",
        "        resp = requests.get(url, headers=headers, params=params, timeout=60)\n",
        "        if resp.status_code == 200:\n",
        "            return resp.json()\n",
        "        if resp.status_code in (429, 500, 502, 503, 504):\n",
        "            wait = sleep_s * attempt * 1.7\n",
        "            print(f\"Backing off ({resp.status_code}). Waiting {wait:.1f}s...\")\n",
        "            time.sleep(wait)\n",
        "            continue\n",
        "        raise RuntimeError(f\"HTTP {resp.status_code}: {resp.text[:300]}\")\n",
        "    raise RuntimeError(\"Failed after max retries.\")\n",
        "\n",
        "def fetch_search(query, offset=0, limit=100):\n",
        "    params = {\n",
        "        \"query\": query,\n",
        "        \"limit\": limit,\n",
        "        \"offset\": offset,\n",
        "        \"fields\": FIELDS,\n",
        "    }\n",
        "    return robust_get(SEARCH_URL, HEADERS, params)\n",
        "\n",
        "def fetch_recent_dcml_multi(target=TARGET, per_query_pages=8):\n",
        "    all_items = []\n",
        "    for q in QUERIES:\n",
        "        print(f\"Query: {q}\")\n",
        "        for page in range(per_query_pages):\n",
        "            data = fetch_search(q, offset=page*PAGE_LIMIT, limit=PAGE_LIMIT)\n",
        "            batch = data.get(\"data\", [])\n",
        "            if not batch:\n",
        "                print(\"  (no more results)\")\n",
        "                break\n",
        "            all_items.extend(batch)\n",
        "            print(f\"  +{len(batch)} (total={len(all_items)})\")\n",
        "            if len(all_items) >= target * 4:  # overfetch for dedupe & trimming\n",
        "                break\n",
        "        if len(all_items) >= target * 4:\n",
        "            break\n",
        "    return all_items\n",
        "\n",
        "def parse_date(s):\n",
        "    if not s:\n",
        "        return None\n",
        "    for fmt in (\"%Y-%m-%d\", \"%Y-%m\", \"%Y\"):\n",
        "        try:\n",
        "            return datetime.strptime(s, fmt)\n",
        "        except:\n",
        "            pass\n",
        "    m = re.search(r\"(19|20)\\d{2}\", s or \"\")\n",
        "    return datetime(int(m.group(0)), 1, 1) if m else None\n",
        "\n",
        "def normalize_text(s):\n",
        "    s = (s or \"\").strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "def to_rows(raw_items):\n",
        "    rows = []\n",
        "    for p in raw_items:\n",
        "        if not isinstance(p, dict):\n",
        "            continue\n",
        "        title = normalize_text(p.get(\"title\"))\n",
        "        abstract = normalize_text(p.get(\"abstract\"))\n",
        "        if not title or not abstract:\n",
        "            continue\n",
        "        ext = p.get(\"externalIds\") or {}\n",
        "        pub_dt = parse_date(p.get(\"publicationDate\"))\n",
        "        if (pub_dt is None) and p.get(\"year\"):\n",
        "            try:\n",
        "                pub_dt = datetime(int(p[\"year\"]), 1, 1)\n",
        "            except:\n",
        "                pub_dt = None\n",
        "        rows.append({\n",
        "            \"title\": title,\n",
        "            \"abstract\": abstract,\n",
        "            \"year\": p.get(\"year\"),\n",
        "            \"venue\": (p.get(\"venue\") or \"\")[:200],\n",
        "            \"publicationDate\": p.get(\"publicationDate\"),\n",
        "            \"pub_dt\": pub_dt,\n",
        "            \"doi\": ext.get(\"DOI\"),\n",
        "            \"arxivId\": ext.get(\"ArXiv\"),\n",
        "            \"url\": p.get(\"url\"),\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "print(\"Step 1: Fetching (multi-query, throttled) ...\")\n",
        "raw = fetch_recent_dcml_multi(TARGET, per_query_pages=10)\n",
        "print(\"Raw items fetched:\", len(raw))\n",
        "\n",
        "df = to_rows(raw)\n",
        "print(\"After to_rows:\", len(df))\n",
        "\n",
        "# Diagnostics\n",
        "print(\"Unique DOIs:\", df[\"doi\"].notna().sum())\n",
        "print(\"Missing publicationDate:\", df[\"publicationDate\"].isna().sum(), \"| Missing year:\", df[\"year\"].isna().sum())\n",
        "print(\"Abstract length ≥ 80:\", (df[\"abstract\"].str.len() >= 80).sum())\n",
        "\n",
        "# Deduplicate by DOI + normalized title\n",
        "df[\"title_norm\"] = df[\"title\"].str.lower().str.replace(r\"\\s+\", \" \", regex=True)\n",
        "df = df.drop_duplicates(subset=[\"doi\", \"title_norm\"], keep=\"first\")\n",
        "\n",
        "# Keep records that have a usable date\n",
        "df = df[df[\"pub_dt\"].notna()].copy()\n",
        "\n",
        "# Filter for computer science venues (this is a basic example, might need refinement)\n",
        "cs_venues = [\"NeurIPS\", \"ICML\", \"ICLR\", \"ACL\", \"EMNLP\", \"ECML PKDD\", \"COLT\", \"UAI\", \"AISTATS\", \"IJCAI\", \"AAAI\", \"ECAI\", \"Artificial Intelligence\", \"Machine Learning\", \"Journal of Machine Learning Research\", \"IEEE Transactions on Pattern Analysis and Machine Intelligence\", \"Data Mining and Knowledge Discovery\", \"Expert Systems with Applications\", \"Knowledge-Based Systems\", \"Pattern Recognition\", \"Neurocomputing\", \"Information Science\"]\n",
        "df = df[df['venue'].str.contains('|'.join(cs_venues), na=False)].copy()\n",
        "\n",
        "# Sort by recency and keep the freshest 1000 with substantial abstracts\n",
        "df = df.sort_values(\"pub_dt\", ascending=False)\n",
        "df_recent = df[df[\"abstract\"].str.len() >= 200].head(TARGET).copy().reset_index(drop=True)\n",
        "\n",
        "print(f\"Kept {len(df_recent)} most recent with nontrivial abstracts from CS venues.\")\n",
        "df_recent = df_recent.drop(columns=[\"title_norm\"])\n",
        "df_recent.to_csv(\"dcml_recent_1000.csv\", index=False)\n",
        "print(\"Saved: dcml_recent_1000.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}