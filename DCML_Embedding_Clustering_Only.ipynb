{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3f4747d0",
      "metadata": {
        "id": "3f4747d0"
      },
      "source": [
        "\n",
        "# Data-Centric ML — **Embedding & Clustering Only**\n",
        "This notebook assumes you already have a CSV (e.g. `dcml_recent_500.csv`) with columns **title** and **abstract**.  \n",
        "It will:\n",
        "1. **Embed** each paper with **SPECTER2** (AllenAI).  \n",
        "2. **Cluster** the embeddings (HDBSCAN by default; optional k-means).  \n",
        "3. Save: `dcml_specter_embeddings.npy` and `dcml_clustered.csv`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edc18bc0",
      "metadata": {
        "id": "edc18bc0"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip -q install pandas numpy tqdm transformers torch scikit-learn hdbscan\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ee32d4a",
      "metadata": {
        "id": "6ee32d4a"
      },
      "source": [
        "\n",
        "## Load your CSV\n",
        "- Option A: Upload via Colab file widget.  \n",
        "- Option B: Mount Drive and set the path accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96217bb5",
      "metadata": {
        "id": "96217bb5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Option A: Upload manually ===\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()  # then set CSV_PATH to the uploaded filename\n",
        "\n",
        "# === Option B: Google Drive ===\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# CSV_PATH = \"/content/drive/MyDrive/dcml/dcml_recent_500.csv\"\n",
        "\n",
        "# === Option C: If already in the runtime ===\n",
        "CSV_PATH = \"dcml_recent_1000.csv\"  # change if needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bb153f9",
      "metadata": {
        "id": "3bb153f9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "required_cols = {\"title\", \"abstract\"}\n",
        "if not required_cols.issubset(set(df.columns.str.lower())):\n",
        "    # try to align case\n",
        "    cols_lower = {c.lower(): c for c in df.columns}\n",
        "    missing = required_cols - set(cols_lower.keys())\n",
        "    if missing:\n",
        "        raise ValueError(f\"CSV must contain columns: {required_cols}. Found: {list(df.columns)}\")\n",
        "    # rename to standard\n",
        "    df = df.rename(columns={cols_lower[\"title\"]:\"title\", cols_lower[\"abstract\"]:\"abstract\"})\n",
        "\n",
        "# Basic cleaning\n",
        "df = df.dropna(subset=[\"title\",\"abstract\"]).copy()\n",
        "df = df[df[\"abstract\"].astype(str).str.len() > 50].reset_index(drop=True)\n",
        "print(df.shape)\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49556922",
      "metadata": {
        "id": "49556922"
      },
      "source": [
        "\n",
        "## Embed with SPECTER2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87a67eca",
      "metadata": {
        "id": "87a67eca"
      },
      "outputs": [],
      "source": [
        "import numpy as np, torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "MODEL_NAME = \"allenai/specter\"  # fallback: \"allenai/specter\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device).eval()\n",
        "\n",
        "def batch_embed(texts, batch_size=8, max_length=512):\n",
        "    vecs = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding with SPECTER2\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n",
        "        with torch.no_grad():\n",
        "            out = model(**inputs).last_hidden_state[:, 0, :]  # [CLS]\n",
        "        vecs.append(out.cpu().numpy())\n",
        "    return np.vstack(vecs)\n",
        "\n",
        "texts = (df[\"title\"].fillna(\"\") + \" \" + df[\"abstract\"].fillna(\"\")).tolist()\n",
        "E = batch_embed(texts, batch_size=8)\n",
        "np.save(\"dcml_specter_embeddings.npy\", E)\n",
        "print(\"Saved embeddings → dcml_specter_embeddings.npy\", E.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40540eb0",
      "metadata": {
        "id": "40540eb0"
      },
      "source": [
        "\n",
        "## Cluster embeddings\n",
        "- **HDBSCAN** (default): finds dense clusters and marks outliers as -1.  \n",
        "- **k-means** (optional): pick a fixed `k` or search for best silhouette.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GdEOl9MuCHVj"
      },
      "id": "GdEOl9MuCHVj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "122f837a"
      },
      "source": [
        "## k-Means Clustering (k=5)\n",
        "\n",
        "### Subtask:\n",
        "Führen Sie k-Means Clustering mit 5 Clustern durch und fügen Sie die Ergebnisse zum DataFrame hinzu."
      ],
      "id": "122f837a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee54a032"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming E contains the embeddings and df is the DataFrame\n",
        "\n",
        "# Standardize (no mean shift) - Ensure X is defined\n",
        "# This part is crucial if X is not guaranteed to be defined from previous cells\n",
        "if 'E' not in locals():\n",
        "    try:\n",
        "        E = np.load(\"dcml_specter_embeddings.npy\")\n",
        "        print(\"Embeddings loaded from dcml_specter_embeddings.npy\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: Embeddings (E) not found. Please run the embedding cell first.\")\n",
        "        E = None # Ensure E is None if loading fails\n",
        "\n",
        "if E is not None:\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    X = scaler.fit_transform(E)\n",
        "\n",
        "    # Perform KMeans clustering with k=5\n",
        "    kmeans_k5 = KMeans(n_clusters=5, n_init=\"auto\", random_state=42)\n",
        "    df['cluster_kmeans_k5'] = kmeans_k5.fit_predict(X)\n",
        "\n",
        "    # Display the count of papers in each new cluster\n",
        "    print(\"KMeans (k=5) Cluster Sizes:\")\n",
        "    print(df['cluster_kmeans_k5'].value_counts().sort_index())\n",
        "\n",
        "    # Optionally, save the updated dataframe\n",
        "    # df.to_csv(\"dcml_clustered_kmeans_k5.csv\", index=False)\n",
        "    # print(\"Saved k-means (k=5) clusters → dcml_clustered_kmeans_k5.csv\")\n",
        "else:\n",
        "    print(\"KMeans clustering skipped because embeddings (E) are not available.\")"
      ],
      "id": "ee54a032",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5e7f757"
      },
      "source": [
        "## Stichwörter für k=5 Cluster\n",
        "\n",
        "### Subtask:\n",
        "Extrahieren Sie die Titel und Abstracts für die k=5 Cluster."
      ],
      "id": "b5e7f757"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4538693d"
      },
      "source": [
        "# Ensure df is loaded\n",
        "if 'df' not in locals() or df is None:\n",
        "    import pandas as pd\n",
        "    # Assuming CSV_PATH is defined earlier\n",
        "    try:\n",
        "        df = pd.read_csv(CSV_PATH)\n",
        "        required_cols = {\"title\", \"abstract\"}\n",
        "        if not required_cols.issubset(set(df.columns.str.lower())):\n",
        "            # try to align case\n",
        "            cols_lower = {c.lower(): c for c in df.columns}\n",
        "            missing = required_cols - set(cols_lower.keys())\n",
        "            if missing:\n",
        "                raise ValueError(f\"CSV must contain columns: {required_cols}. Found: {list(df.columns)}\")\n",
        "            # rename to standard\n",
        "            df = df.rename(columns={cols_lower[\"title\"]:\"title\", cols_lower[\"abstract\"]:\"abstract\"})\n",
        "\n",
        "        # Basic cleaning\n",
        "        df = df.dropna(subset=[\"title\",\"abstract\"]).copy()\n",
        "        df = df[df[\"abstract\"].astype(str).str.len() > 50].reset_index(drop=True)\n",
        "        print(\"DataFrame loaded and cleaned.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or cleaning DataFrame: {e}\")\n",
        "        df = None # Ensure df is None if loading fails\n",
        "\n",
        "# Ensure 'cluster_kmeans_k5' column exists. This part assumes the KMeans k=5 clustering cell was run successfully.\n",
        "if df is not None and 'cluster_kmeans_k5' not in df.columns:\n",
        "    print(\"Warning: 'cluster_kmeans_k5' column not found in DataFrame. Please run the KMeans k=5 clustering cell.\")\n",
        "    # Attempt to run KMeans k=5 if possible (requires X to be defined)\n",
        "    if 'X' in locals() and X is not None:\n",
        "         try:\n",
        "            from sklearn.cluster import KMeans\n",
        "            kmeans_k5 = KMeans(n_clusters=5, n_init=\"auto\", random_state=42)\n",
        "            df['cluster_kmeans_k5'] = kmeans_k5.fit_predict(X)\n",
        "            print(\"KMeans (k=5) clustering performed.\")\n",
        "         except Exception as e:\n",
        "            print(f\"Error performing KMeans k=5 clustering: {e}\")\n",
        "\n",
        "\n",
        "if df is not None and 'cluster_kmeans_k5' in df.columns:\n",
        "    cluster_texts_k5 = {}\n",
        "    unique_kmeans_k5_clusters = df['cluster_kmeans_k5'].unique()\n",
        "\n",
        "    for cluster_id in sorted(unique_kmeans_k5_clusters):\n",
        "        cluster_df = df[df['cluster_kmeans_k5'] == cluster_id]\n",
        "        cluster_texts_k5[cluster_id] = (cluster_df['title'].fillna(\"\") + \" \" + cluster_df['abstract'].fillna(\"\")).tolist()\n",
        "\n",
        "    # Display a sample of the extracted texts for verification\n",
        "    print(\"Sample texts for KMeans k=5 clusters:\")\n",
        "    for cluster_id, texts in cluster_texts_k5.items():\n",
        "        print(f\"Cluster {cluster_id} ({len(texts)} documents):\")\n",
        "        for i, text in enumerate(texts[:2]): # Print first 2 texts\n",
        "            print(f\"  - {text[:150]}...\")\n",
        "        print(\"-\" * 20)\n",
        "        if cluster_id > 2: # Limit output for brevity\n",
        "            break\n",
        "else:\n",
        "    print(\"DataFrame or 'cluster_kmeans_k5' column not available. Cannot extract cluster texts.\")"
      ],
      "id": "4538693d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c74e1c55"
      },
      "source": [
        "### Subtask:\n",
        "Bereinigen und tokenisieren Sie den Text für die k=5 Cluster."
      ],
      "id": "c74e1c55"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15cfdd11"
      },
      "source": [
        "# Reuse the preprocess_text function defined earlier\n",
        "\n",
        "preprocessed_cluster_texts_k5 = {}\n",
        "\n",
        "if 'cluster_texts_k5' in locals() and cluster_texts_k5:\n",
        "    for cluster_id, texts in cluster_texts_k5.items():\n",
        "        preprocessed_cluster_texts_k5[cluster_id] = [preprocess_text(text) for text in texts]\n",
        "\n",
        "    # Replace original cluster_texts_k5 with preprocessed texts\n",
        "    cluster_texts_k5 = preprocessed_cluster_texts_k5\n",
        "\n",
        "    # Display a sample of the preprocessed texts for verification\n",
        "    print(\"Sample preprocessed texts for KMeans k=5 clusters:\")\n",
        "    for cluster_id, texts in cluster_texts_k5.items():\n",
        "        print(f\"Cluster {cluster_id} ({len(texts)} documents):\")\n",
        "        for i, text_tokens in enumerate(texts[:2]): # Print first 2 preprocessed texts\n",
        "            print(f\"  - {' '.join(text_tokens[:20])}...\") # Print first 20 tokens\n",
        "        print(\"-\" * 20)\n",
        "        if cluster_id > 2: # Limit output for brevity\n",
        "            break\n",
        "else:\n",
        "    print(\"Variable 'cluster_texts_k5' not found or is empty. Please run the cell to extract texts by cluster for k=5 (e.g., cell 4538693d) before running this cell.\")"
      ],
      "id": "15cfdd11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bf65762"
      },
      "source": [
        "### Subtask:\n",
        "Verwenden Sie TF-IDF, um die wichtigsten Stichwörter für die k=5 Cluster zu identifizieren."
      ],
      "id": "0bf65762"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c54d0bb3"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np # Ensure numpy is imported\n",
        "\n",
        "# Ensure cluster_texts_k5 is populated from the previous step\n",
        "if 'cluster_texts_k5' in locals() and cluster_texts_k5:\n",
        "    cluster_keywords_k5 = {}\n",
        "    N = 10  # Number of top keywords to extract per cluster\n",
        "\n",
        "    # Prepare documents for TF-IDF: each cluster is a single \"document\" of joined tokens\n",
        "    cluster_docs_k5 = [\" \".join([\" \".join(doc) for doc in cluster_texts_k5[cluster_id]]) for cluster_id in sorted(cluster_texts_k5.keys())]\n",
        "\n",
        "    # Initialize TF-IDF Vectorizer\n",
        "    # We can use the preprocessed tokens directly by setting tokenizer and preprocessor to None\n",
        "    # and ensuring the input is a list of strings (which cluster_docs_k5 is after joining tokens)\n",
        "    # min_df and max_df can help filter out too rare or too common terms\n",
        "    vectorizer_k5 = TfidfVectorizer(min_df=2, max_df=0.95)\n",
        "\n",
        "    # Fit and transform the cluster documents\n",
        "    try:\n",
        "        tfidf_matrix_k5 = vectorizer_k5.fit_transform(cluster_docs_k5)\n",
        "\n",
        "        # Check if vocabulary is empty after fitting\n",
        "        if not vectorizer_k5.get_feature_names_out().size:\n",
        "             print(\"ValueError: empty vocabulary; perhaps the documents only contain stop words or are too short after preprocessing.\")\n",
        "        else:\n",
        "            # Get feature names (words)\n",
        "            feature_names_k5 = vectorizer_k5.get_feature_names_out()\n",
        "\n",
        "            # Extract top keywords for each cluster\n",
        "            for i, cluster_id in enumerate(sorted(cluster_texts_k5.keys())):\n",
        "                # Get TF-IDF scores for the current cluster\n",
        "                tfidf_scores_k5 = tfidf_matrix_k5[i].toarray().flatten()\n",
        "\n",
        "                # Get indices of top N scores\n",
        "                top_n_indices_k5 = tfidf_scores_k5.argsort()[-N:][::-1]\n",
        "\n",
        "                # Get top N keywords and their scores\n",
        "                top_keywords_with_scores_k5 = [(feature_names_k5[j], tfidf_scores_k5[j]) for j in top_n_indices_k5]\n",
        "\n",
        "                # Store just the keywords\n",
        "                cluster_keywords_k5[cluster_id] = [keyword for keyword, score in top_keywords_with_scores_k5]\n",
        "\n",
        "            # Print the top keywords for each cluster\n",
        "            print(\"\\nTop Keywords per Cluster (KMeans k=5):\")\n",
        "            for cluster_id, keywords in cluster_keywords_k5.items():\n",
        "                print(f\"Cluster {cluster_id}: {', '.join(keywords)}\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"Error during TF-IDF calculation: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"cluster_texts_k5 is not available or empty. Cannot perform TF-IDF.\")"
      ],
      "id": "c54d0bb3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4905649"
      },
      "source": [
        "## Visualisierung für k=5 Cluster\n",
        "\n",
        "### Subtask:\n",
        "Visualisieren Sie die k=5 Cluster mit UMAP."
      ],
      "id": "d4905649"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ca04a7b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import pandas as pd # Ensure pandas is imported\n",
        "\n",
        "# Assuming df contains 'umap_x', 'umap_y' and 'cluster_kmeans_k5' columns\n",
        "\n",
        "# Ensure df is loaded and umap columns exist\n",
        "if 'df' not in locals() or df is None:\n",
        "    print(\"DataFrame not loaded. Please run the data loading and UMAP reduction cells.\")\n",
        "elif 'umap_x' not in df.columns or 'umap_y' not in df.columns:\n",
        "     print(\"UMAP columns ('umap_x', 'umap_y') not found in DataFrame. Please run the UMAP reduction cell.\")\n",
        "elif 'cluster_kmeans_k5' not in df.columns:\n",
        "     print(\"'cluster_kmeans_k5' column not found in DataFrame. Please run the KMeans k=5 clustering cell.\")\n",
        "else:\n",
        "    # Visualize KMeans k=5 clusters using a categorical palette with shuffled colors\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    num_kmeans_k5_clusters = df['cluster_kmeans_k5'].nunique()\n",
        "    # Use a categorical palette and shuffle the colors\n",
        "    palette_k5 = sns.color_palette(\"hsv\", num_kmeans_k5_clusters)\n",
        "    random.seed(42) # for reproducibility\n",
        "    random.shuffle(palette_k5)\n",
        "\n",
        "    sns.scatterplot(data=df, x='umap_x', y='umap_y', hue='cluster_kmeans_k5', palette=palette_k5, s=10, legend='full')\n",
        "    plt.title('KMeans Clusters (k=5) visualized with UMAP (Categorical Colors - Shuffled)')\n",
        "    plt.xlabel('UMAP 1')\n",
        "    plt.ylabel('UMAP 2')\n",
        "    plt.show()"
      ],
      "id": "1ca04a7b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e62d4798"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming E contains the embeddings and df is the DataFrame\n",
        "\n",
        "# Standardize (no mean shift) - Ensure X is defined\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X = scaler.fit_transform(E)\n",
        "\n",
        "\n",
        "# Perform KMeans clustering with k=5\n",
        "kmeans_k5 = KMeans(n_clusters=5, n_init=\"auto\", random_state=42)\n",
        "df['cluster_kmeans_k5'] = kmeans_k5.fit_predict(X)\n",
        "\n",
        "# Display the count of papers in each new cluster\n",
        "print(\"KMeans (k=5) Cluster Sizes:\")\n",
        "print(df['cluster_kmeans_k5'].value_counts().sort_index())\n",
        "\n",
        "# Optionally, save the updated dataframe\n",
        "# df.to_csv(\"dcml_clustered_kmeans_k5.csv\", index=False)\n",
        "# print(\"Saved k-means (k=5) clusters → dcml_clustered_kmeans_k5.csv\")"
      ],
      "id": "e62d4798",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4b78c27",
      "metadata": {
        "id": "f4b78c27"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import hdbscan\n",
        "\n",
        "# Standardize (no mean shift)\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X = scaler.fit_transform(E)\n",
        "\n",
        "# HDBSCAN\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=15, min_samples=10, metric=\"euclidean\")\n",
        "labels = clusterer.fit_predict(X)\n",
        "df[\"cluster\"] = labels\n",
        "\n",
        "n_clusters = len(set(labels) - {-1})\n",
        "noise_frac = (labels == -1).mean()\n",
        "print(f\"HDBSCAN → clusters: {n_clusters} (noise: {noise_frac:.2%})\")\n",
        "\n",
        "df.to_csv(\"dcml_clustered.csv\", index=False)\n",
        "print(\"Saved clusters → dcml_clustered.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e93c1d21",
      "metadata": {
        "id": "e93c1d21"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "best_k, best_score = None, -1\n",
        "for k in range(6, 21):\n",
        "    km = KMeans(n_clusters=k, n_init=\"auto\", random_state=42).fit(X)\n",
        "    score = silhouette_score(X, km.labels_)\n",
        "    if score > best_score:\n",
        "        best_k, best_score = k, score\n",
        "print(f\"Best k by silhouette: {best_k} (score={best_score:.3f})\")\n",
        "km = KMeans(n_clusters=best_k or 10, n_init=\"auto\", random_state=42).fit(X)\n",
        "df['cluster_kmeans'] = km.labels_\n",
        "df.to_csv(\"dcml_clustered_kmeans.csv\", index=False)\n",
        "print(\"Saved k-means clusters → dcml_clustered_kmeans.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88228a31"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# Assuming df contains 'cluster' and 'cluster_kmeans' columns and E contains the embeddings\n",
        "\n",
        "def get_cluster_stats_and_representatives(dataframe, embeddings, cluster_column):\n",
        "    \"\"\"\n",
        "    Calculates statistics for each cluster and finds a representative paper.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pd.DataFrame): DataFrame with cluster labels.\n",
        "        embeddings (np.ndarray): Array of embeddings.\n",
        "        cluster_column (str): Name of the column containing cluster labels.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are cluster labels and values are dictionaries\n",
        "              containing 'size' and 'representative_paper' (title and abstract).\n",
        "    \"\"\"\n",
        "    cluster_info = {}\n",
        "    unique_clusters = dataframe[cluster_column].unique()\n",
        "\n",
        "    for cluster_id in sorted(unique_clusters):\n",
        "        if cluster_id == -1 and cluster_column == 'cluster':\n",
        "            # Skip noise cluster for HDBSCAN representative\n",
        "            continue\n",
        "\n",
        "        cluster_df = dataframe[dataframe[cluster_column] == cluster_id].copy()\n",
        "        cluster_indices = cluster_df.index.tolist()\n",
        "        cluster_embeddings = embeddings[cluster_indices]\n",
        "\n",
        "        # Calculate cluster size\n",
        "        cluster_size = len(cluster_df)\n",
        "\n",
        "        representative_paper = None\n",
        "        if cluster_size > 0:\n",
        "            # Calculate centroid of the cluster embeddings\n",
        "            centroid = np.mean(cluster_embeddings, axis=0)\n",
        "\n",
        "            # Find the paper closest to the centroid\n",
        "            distances_to_centroid = cdist([centroid], cluster_embeddings, metric='euclidean')[0]\n",
        "            closest_index_in_cluster = np.argmin(distances_to_centroid)\n",
        "            original_index = cluster_indices[closest_index_in_cluster]\n",
        "            representative_paper = {\n",
        "                \"title\": dataframe.loc[original_index, 'title'],\n",
        "                \"abstract\": dataframe.loc[original_index, 'abstract']\n",
        "            }\n",
        "\n",
        "        cluster_info[cluster_id] = {\n",
        "            \"size\": cluster_size,\n",
        "            \"representative_paper\": representative_paper\n",
        "        }\n",
        "\n",
        "    return cluster_info\n",
        "\n",
        "# Get stats and representatives for HDBSCAN clusters\n",
        "hdbscan_cluster_stats = get_cluster_stats_and_representatives(df, E, 'cluster')\n",
        "print(\"HDBSCAN Cluster Statistics and Representatives:\")\n",
        "for cluster_id, info in hdbscan_cluster_stats.items():\n",
        "    print(f\"Cluster {cluster_id}: Size = {info['size']}\")\n",
        "    if info['representative_paper']:\n",
        "        print(f\"  Representative Paper Title: {info['representative_paper']['title']}\")\n",
        "        print(f\"  Representative Paper Abstract: {info['representative_paper']['abstract'][:200]}...\") # Print first 200 chars\n",
        "    else:\n",
        "        print(\"  No representative paper (cluster is empty or noise)\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "\n",
        "# Get stats and representatives for KMeans clusters\n",
        "kmeans_cluster_stats = get_cluster_stats_and_representatives(df, E, 'cluster_kmeans')\n",
        "print(\"\\nKMeans Cluster Statistics and Representatives:\")\n",
        "for cluster_id, info in kmeans_cluster_stats.items():\n",
        "    print(f\"Cluster {cluster_id}: Size = {info['size']}\")\n",
        "    if info['representative_paper']:\n",
        "        print(f\"  Representative Paper Title: {info['representative_paper']['title']}\")\n",
        "        print(f\"  Representative Paper Abstract: {info['representative_paper']['abstract'][:200]}...\") # Print first 200 chars\n",
        "    else:\n",
        "        print(\"  No representative paper (cluster is empty)\")\n",
        "    print(\"-\" * 20)"
      ],
      "id": "88228a31",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "438f541a"
      },
      "source": [
        "!pip install umap-learn matplotlib seaborn"
      ],
      "id": "438f541a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcf768a2"
      },
      "source": [
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Load embeddings if not already in memory (optional, assuming E is available)\n",
        "# E = np.load(\"dcml_specter_embeddings.npy\")\n",
        "\n",
        "# Reduce dimensionality with UMAP\n",
        "reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "umap_embeddings = reducer.fit_transform(E)\n",
        "\n",
        "# Add UMAP coordinates to the dataframe\n",
        "df['umap_x'] = umap_embeddings[:, 0]\n",
        "df['umap_y'] = umap_embeddings[:, 1]\n",
        "\n",
        "# Visualize HDBSCAN clusters (using a simple palette as there's only noise)\n",
        "plt.figure(figsize=(12, 8))\n",
        "# Since HDBSCAN resulted in only noise (-1), a simple color distinguishes noise from potential future clusters\n",
        "sns.scatterplot(data=df, x='umap_x', y='umap_y', hue='cluster', palette='viridis', s=10, legend='full')\n",
        "plt.title('HDBSCAN Clusters visualized with UMAP')\n",
        "plt.xlabel('UMAP 1')\n",
        "plt.ylabel('UMAP 2')\n",
        "plt.show()\n",
        "\n",
        "# Add a new column for KMeans clusters shifted by 1 for plotting\n",
        "df['cluster_kmeans_plot'] = df['cluster_kmeans'] + 1\n",
        "\n",
        "# Visualize KMeans clusters using a categorical palette with shuffled colors (as requested previously)\n",
        "plt.figure(figsize=(12, 8))\n",
        "num_kmeans_clusters = df['cluster_kmeans_plot'].nunique()\n",
        "# Use a categorical palette and shuffle the colors\n",
        "palette = sns.color_palette(\"hsv\", num_kmeans_clusters)\n",
        "random.seed(42) # for reproducibility\n",
        "random.shuffle(palette)\n",
        "\n",
        "sns.scatterplot(data=df, x='umap_x', y='umap_y', hue='cluster_kmeans_plot', palette=palette, s=10, legend='full')\n",
        "plt.title('KMeans Clusters (k=7) visualized with UMAP') # Changed title\n",
        "plt.xlabel('UMAP 1')\n",
        "plt.ylabel('UMAP 2')\n",
        "plt.show()"
      ],
      "id": "fcf768a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78b63dfc"
      },
      "source": [
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Assuming E contains the embeddings and df contains 'cluster' and 'cluster_kmeans'\n",
        "\n",
        "# Reduce dimensionality with UMAP if not already done\n",
        "if 'umap_x' not in df.columns or 'umap_y' not in df.columns:\n",
        "    print(\"Performing UMAP dimensionality reduction...\")\n",
        "    reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "    umap_embeddings = reducer.fit_transform(E)\n",
        "    df['umap_x'] = umap_embeddings[:, 0]\n",
        "    df['umap_y'] = umap_embeddings[:, 1]\n",
        "    print(\"UMAP reduction complete.\")\n",
        "else:\n",
        "    print(\"UMAP coordinates already found in DataFrame.\")\n",
        "\n",
        "\n",
        "# Visualize HDBSCAN clusters\n",
        "plt.figure(figsize=(14, 10)) # Increased figure size\n",
        "# Since HDBSCAN resulted in only noise (-1), a simple color distinguishes noise from potential future clusters\n",
        "sns.scatterplot(data=df, x='umap_x', y='umap_y', hue='cluster', palette='viridis', s=10, legend='full')\n",
        "plt.title('HDBSCAN Clusters visualized with UMAP')\n",
        "plt.xlabel('UMAP 1')\n",
        "plt.ylabel('UMAP 2')\n",
        "plt.show()\n",
        "\n",
        "# Add a new column for KMeans clusters shifted by 1 for plotting\n",
        "df['cluster_kmeans_plot'] = df['cluster_kmeans'] + 1\n",
        "\n",
        "# Visualize KMeans clusters using a categorical palette with shuffled colors (as requested)\n",
        "plt.figure(figsize=(14, 10)) # Increased figure size\n",
        "num_kmeans_clusters = df['cluster_kmeans_plot'].nunique()\n",
        "# Use a different categorical palette with hopefully higher contrast and shuffle the colors\n",
        "palette = sns.color_palette(\"tab20\", num_kmeans_clusters) # Using tab20 palette\n",
        "random.seed(42) # for reproducibility\n",
        "random.shuffle(palette)\n",
        "\n",
        "sns.scatterplot(data=df, x='umap_x', y='umap_y', hue='cluster_kmeans_plot', palette=palette, s=10, legend='full')\n",
        "plt.title('KMeans Clusters (k=7) visualized with UMAP') # Changed title\n",
        "plt.xlabel('UMAP 1')\n",
        "plt.ylabel('UMAP 2')\n",
        "plt.show()"
      ],
      "id": "78b63dfc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d597f275"
      },
      "source": [
        "# Task\n",
        "Extract representative keywords for each cluster from the provided text data."
      ],
      "id": "d597f275"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cfb5894"
      },
      "source": [
        "## Textdaten nach clustern sammeln\n",
        "\n",
        "### Subtask:\n",
        "Extrahieren Sie die Titel und Abstracts für jedes Cluster.\n"
      ],
      "id": "6cfb5894"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce45f313"
      },
      "source": [
        "cluster_texts = {}\n",
        "unique_kmeans_clusters = df['cluster_kmeans'].unique()\n",
        "\n",
        "for cluster_id in sorted(unique_kmeans_clusters):\n",
        "    cluster_df = df[df['cluster_kmeans'] == cluster_id]\n",
        "    cluster_texts[cluster_id] = (cluster_df['title'].fillna(\"\") + \" \" + cluster_df['abstract'].fillna(\"\")).tolist()\n",
        "\n",
        "# Display a sample of the extracted texts for verification\n",
        "for cluster_id, texts in cluster_texts.items():\n",
        "    print(f\"Cluster {cluster_id} ({len(texts)} documents):\")\n",
        "    for i, text in enumerate(texts[:2]): # Print first 2 texts\n",
        "        print(f\"  - {text[:150]}...\")\n",
        "    print(\"-\" * 20)\n",
        "    if cluster_id > 2: # Limit output for brevity\n",
        "        break"
      ],
      "id": "ce45f313",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67702953"
      },
      "source": [
        "## Textvorverarbeitung\n",
        "\n",
        "### Subtask:\n",
        "Bereinigen und tokenisieren Sie den Text für jedes Cluster.\n"
      ],
      "id": "67702953"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddc697d5"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK data (if not already downloaded)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Converts text to lowercase, removes punctuation, tokenizes, removes stopwords, and lemmatizes.\"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Tokenize text\n",
        "    tokens = text.split()\n",
        "    # Remove stop words and lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing to texts in each cluster\n",
        "preprocessed_cluster_texts = {}\n",
        "for cluster_id, texts in cluster_texts.items():\n",
        "    preprocessed_cluster_texts[cluster_id] = [preprocess_text(text) for text in texts]\n",
        "\n",
        "# Replace original cluster_texts with preprocessed texts\n",
        "cluster_texts = preprocessed_cluster_texts\n",
        "\n",
        "# Display a sample of the preprocessed texts for verification\n",
        "for cluster_id, texts in cluster_texts.items():\n",
        "    print(f\"Cluster {cluster_id} ({len(texts)} documents):\")\n",
        "    for i, text_tokens in enumerate(texts[:2]): # Print first 2 preprocessed texts\n",
        "        print(f\"  - {' '.join(text_tokens[:20])}...\") # Print first 20 tokens\n",
        "    print(\"-\" * 20)\n",
        "    if cluster_id > 2: # Limit output for brevity\n",
        "        break"
      ],
      "id": "ddc697d5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgZlVfrLzoar"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK data (if not already downloaded)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Converts text to lowercase, removes punctuation, tokenizes, removes stopwords, and lemmatizes.\"\"\"\n",
        "    # Check if text is already a list (likely preprocessed)\n",
        "    if isinstance(text, list):\n",
        "        return text\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Tokenize text\n",
        "    tokens = text.split()\n",
        "    # Remove stop words and lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing to texts in each cluster\n",
        "preprocessed_cluster_texts = {}\n",
        "\n",
        "if 'cluster_texts' in locals() and cluster_texts:\n",
        "    for cluster_id, texts in cluster_texts.items():\n",
        "        preprocessed_cluster_texts[cluster_id] = [preprocess_text(text) for text in texts]\n",
        "\n",
        "    # Replace original cluster_texts with preprocessed texts\n",
        "    cluster_texts = preprocessed_cluster_texts\n",
        "\n",
        "    # Display a sample of the preprocessed texts for verification\n",
        "    print(\"Sample preprocessed texts for KMeans clusters:\")\n",
        "    for cluster_id, texts in cluster_texts.items():\n",
        "        print(f\"Cluster {cluster_id} ({len(texts)} documents):\")\n",
        "        for i, text_tokens in enumerate(texts[:2]): # Print first 2 preprocessed texts\n",
        "            print(f\"  - {' '.join(text_tokens[:20])}...\") # Print first 20 tokens\n",
        "        print(\"-\" * 20)\n",
        "        if cluster_id > 2: # Limit output for brevity\n",
        "            break\n",
        "else:\n",
        "    print(\"Variable 'cluster_texts' not found or is empty. Please run the cell to extract texts by cluster (e.g., cell ce45f313) before running this cell.\")"
      ],
      "id": "CgZlVfrLzoar",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(E)"
      ],
      "metadata": {
        "id": "jpVQkYu3wKSX"
      },
      "id": "jpVQkYu3wKSX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a666d22"
      },
      "source": [
        "## Stichwortextraktion\n",
        "\n",
        "### Subtask:\n",
        "Verwenden Sie eine Methode wie TF-IDF oder Häufigkeitsanalyse, um die wichtigsten Stichwörter für jedes Cluster zu identifizieren.\n"
      ],
      "id": "5a666d22"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0893fd0"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "cluster_keywords = {}\n",
        "N = 10  # Number of top keywords to extract per cluster\n",
        "\n",
        "# Prepare documents for TF-IDF: each cluster is a single \"document\" of joined tokens\n",
        "cluster_docs = [\" \".join([\" \".join(doc) for doc in cluster_texts[cluster_id]]) for cluster_id in sorted(cluster_texts.keys())]\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "# We can use the preprocessed tokens directly by setting tokenizer and preprocessor to None\n",
        "# and ensuring the input is a list of strings (which cluster_docs is after joining tokens)\n",
        "# min_df and max_df can help filter out too rare or too common terms\n",
        "vectorizer = TfidfVectorizer(min_df=2, max_df=0.95)\n",
        "\n",
        "# Fit and transform the cluster documents\n",
        "tfidf_matrix = vectorizer.fit_transform(cluster_docs)\n",
        "\n",
        "# Get feature names (words)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Extract top keywords for each cluster\n",
        "for i, cluster_id in enumerate(sorted(cluster_texts.keys())):\n",
        "    # Get TF-IDF scores for the current cluster\n",
        "    tfidf_scores = tfidf_matrix[i].toarray().flatten()\n",
        "\n",
        "    # Get indices of top N scores\n",
        "    top_n_indices = tfidf_scores.argsort()[-N:][::-1]\n",
        "\n",
        "    # Get top N keywords and their scores\n",
        "    top_keywords_with_scores = [(feature_names[j], tfidf_scores[j]) for j in top_n_indices]\n",
        "\n",
        "    # Store just the keywords\n",
        "    cluster_keywords[cluster_id] = [keyword for keyword, score in top_keywords_with_scores]\n",
        "\n",
        "# Print the top keywords for each cluster\n",
        "print(\"Top Keywords per Cluster (KMeans):\")\n",
        "for cluster_id, keywords in cluster_keywords.items():\n",
        "    print(f\"Cluster {cluster_id}: {', '.join(keywords)}\")\n"
      ],
      "id": "e0893fd0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "023c3c54"
      },
      "source": [
        "## Ergebnisse anzeigen\n",
        "\n",
        "### Subtask:\n",
        "Präsentieren Sie die gefundenen Stichwörter für jedes Cluster.\n"
      ],
      "id": "023c3c54"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "669a8164"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the cluster_keywords dictionary and print the cluster ID and its keywords.\n",
        "\n"
      ],
      "id": "669a8164"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e755a990"
      },
      "source": [
        "# Present the extracted keywords for each cluster\n",
        "print(\"Extracted Keywords for Each Cluster (KMeans):\")\n",
        "for cluster_id, keywords in cluster_keywords.items():\n",
        "    print(f\"Cluster {cluster_id}: {', '.join(keywords)}\")"
      ],
      "id": "e755a990",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}